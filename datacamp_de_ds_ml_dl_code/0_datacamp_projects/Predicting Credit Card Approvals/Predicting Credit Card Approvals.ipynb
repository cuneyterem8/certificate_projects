{"cells":[{"source":"Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n![Credit card being held in hand](credit_card.jpg)\n\nYou have been provided with a small subset of the credit card applications a bank receives. The dataset has been loaded as a Pandas DataFrame for you. You will start from there. ","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","metadata":{"executionCancelledAt":null,"executionTime":35,"lastExecutedAt":1704314418995,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","outputsMetadata":{"0":{"height":209,"type":"dataFrame"}}},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":47,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"string"},{"name":"1","type":"string"},{"name":"2","type":"number"},{"name":"3","type":"string"},{"name":"4","type":"string"},{"name":"5","type":"string"},{"name":"6","type":"string"},{"name":"7","type":"number"},{"name":"8","type":"string"},{"name":"9","type":"string"},{"name":"10","type":"integer"},{"name":"11","type":"string"},{"name":"12","type":"string"},{"name":"13","type":"string"},{"name":"14","type":"integer"},{"name":"15","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":["b","a","a","b","b"],"1":["30.83","58.67","24.50","27.83","20.17"],"2":[0,4.46,0.5,1.54,5.625],"3":["u","u","u","u","u"],"4":["g","g","g","g","g"],"5":["w","q","q","w","w"],"6":["v","h","h","v","v"],"7":[1.25,3.04,1.5,3.75,1.71],"8":["t","t","t","t","t"],"9":["t","t","f","t","f"],"10":[1,6,0,5,0],"11":["f","f","f","t","f"],"12":["g","g","g","g","s"],"13":["00202","00043","00280","00100","00120"],"14":[0,560,824,3,0],"15":["+","+","+","+","+"],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b</td>\n      <td>30.83</td>\n      <td>0.000</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.25</td>\n      <td>t</td>\n      <td>t</td>\n      <td>1</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00202</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>58.67</td>\n      <td>4.460</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>3.04</td>\n      <td>t</td>\n      <td>t</td>\n      <td>6</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00043</td>\n      <td>560</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>24.50</td>\n      <td>0.500</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>1.50</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00280</td>\n      <td>824</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b</td>\n      <td>27.83</td>\n      <td>1.540</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>3.75</td>\n      <td>t</td>\n      <td>t</td>\n      <td>5</td>\n      <td>t</td>\n      <td>g</td>\n      <td>00100</td>\n      <td>3</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b</td>\n      <td>20.17</td>\n      <td>5.625</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.71</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>f</td>\n      <td>s</td>\n      <td>00120</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":47}]},{"source":"cc_apps.info()","metadata":{"executionCancelledAt":null,"executionTime":57,"lastExecutedAt":1704314419052,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"cc_apps.info()","outputsMetadata":{"0":{"height":486,"type":"stream"}}},"id":"adc05611-4f58-4d58-a767-ca973e3cc331","cell_type":"code","execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 16 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    object \n 13  13      690 non-null    object \n 14  14      690 non-null    int64  \n 15  15      690 non-null    object \ndtypes: float64(2), int64(2), object(12)\nmemory usage: 86.4+ KB\n"}]},{"source":"cc_apps.describe()","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1704314419104,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"cc_apps.describe()"},"cell_type":"code","id":"f286ce4e-86f8-42b0-bbaa-6ef3e89c2de4","execution_count":49,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"string"},{"name":"2","type":"number"},{"name":"7","type":"number"},{"name":"10","type":"number"},{"name":"14","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"2":[690,4.7587246377,4.9781632485,0,1,2.75,7.2075,28],"7":[690,2.2234057971,3.3465133593,0,0.165,1,2.625,28.5],"10":[690,2.4,4.8629400342,0,0,0,3,67],"14":[690,1017.3855072464,5210.1025983027,0,0,5,395.5,100000],"index":["count","mean","std","min","25%","50%","75%","max"]}},"total_rows":8,"truncation_type":null},"text/plain":"               2           7          10             14\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2</th>\n      <th>7</th>\n      <th>10</th>\n      <th>14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>690.000000</td>\n      <td>690.000000</td>\n      <td>690.00000</td>\n      <td>690.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.758725</td>\n      <td>2.223406</td>\n      <td>2.40000</td>\n      <td>1017.385507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.978163</td>\n      <td>3.346513</td>\n      <td>4.86294</td>\n      <td>5210.102598</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.165000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.750000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.207500</td>\n      <td>2.625000</td>\n      <td>3.00000</td>\n      <td>395.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.000000</td>\n      <td>28.500000</td>\n      <td>67.00000</td>\n      <td>100000.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":49}]},{"source":"cc_apps = cc_apps.drop([11, 13], axis=1)\n\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n\n# Replace the '?'s with NaN in the train and test sets\ncc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\ncc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n\n# Impute the missing values with mean imputation\ncc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\ncc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n\nfor col in cc_apps_train_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_train_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n\n# Convert the categorical features in the train and test sets independently\ncc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\ncc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n)\n\n# Segregate features and labels into separate variables\nX_train, y_train = (\n    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n)\nX_test, y_test = (\n    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n)\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1704314419167,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"cc_apps = cc_apps.drop([11, 13], axis=1)\n\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n\n# Replace the '?'s with NaN in the train and test sets\ncc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\ncc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n\n# Impute the missing values with mean imputation\ncc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\ncc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n\nfor col in cc_apps_train_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_train_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n\n# Convert the categorical features in the train and test sets independently\ncc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\ncc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n)\n\n# Segregate features and labels into separate variables\nX_train, y_train = (\n    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n)\nX_test, y_test = (\n    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n)\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)"},"cell_type":"code","id":"d6025246-f3db-4218-8809-184d0b6e5ec1","execution_count":50,"outputs":[]},{"source":"# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_test, y_pred))","metadata":{"executionCancelledAt":null,"executionTime":91,"lastExecutedAt":1704314419258,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_test, y_pred))","outputsMetadata":{"0":{"height":58,"type":"stream"}}},"cell_type":"code","id":"7880b768-1828-4cea-afc7-3c0cf72f3292","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"[[103   0]\n [  0 125]]\n"}]},{"source":"# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\n    \"Accuracy of logistic regression classifier: \",\n    best_model.score(rescaledX_test, y_test),\n)","metadata":{"executionCancelledAt":null,"executionTime":3995,"lastExecutedAt":1704314423253,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\n    \"Accuracy of logistic regression classifier: \",\n    best_model.score(rescaledX_test, y_test),\n)"},"cell_type":"code","id":"4f331c8d-6ab2-4536-ae5e-336c051c98bd","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":"Best: 1.000000 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  1.0\n"}]},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1704314423300,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"7fcb1a6f-e289-4f2d-9526-f2ab44f4b226","execution_count":52,"outputs":[]},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1704314423352,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"1b4c9b40-7aa3-4338-be63-6b6167a06a13","execution_count":52,"outputs":[]},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1704314423400,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"40d5c731-9ac6-42b6-8b8f-960f4192d429","execution_count":52,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}